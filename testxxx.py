import pytest
from pyspark.sql import DataFrame, SparkSession
from src.advanced_field_monitoring.data_loader.extract.tables.TableInterface import (
    ArgType,
)
from src.advanced_field_monitoring.data_loader.shared import Constants as C
from src.advanced_field_monitoring.data_loader.shared.ClearComputationGraph import (
    clear_computation_graph,
)
from src.advanced_field_monitoring.data_loader.shared.FileUtils import read_csv

from .filter_test_data import (
    battery_diag_cell_testdata,
    battery_diag_testdata,
    engineering_model_testdata,
    forecast_diag_ml_testdata,
    forecast_diag_testdata,
    forecast_validation_testdata,
    model_overview_testdata,
    read_test_data,
)


@pytest.fixture(scope="session")
def diagnoses_for_test_df(
    spark: SparkSession, args: ArgType, testbed: str
) -> DataFrame:
    file = "diagnoses_for_test"
    # The spec dataframe used to contain a column mock_vehicle_id for purely human
    # consumption, but it fell out of sync with the autogenerated pseudonymous ids.
    diagnoses_for_test_df = read_test_data(spark, args, file, encrypted=True)
    # when determining suitable test data, you usually need the real vehicle_id.
    # Most of them are in xev.blk_hist, but some only in xev.add_data_conv or xev.dtc.
    return diagnoses_for_test_df

# ######## CSV seed data dataframes ###########################################
# don't separate out the csv handling stuff. It slows down the test suite
# by 0.2 seconds (or 6-8 seconds if 200 parititons). Why is that?
@pytest.fixture(scope="session")
def eng_model_specification_df(
    spark: SparkSession, args: ArgType, testbed: str
) -> DataFrame:
    path = args.artifacts_home + "src_data/eng_model_specification.csv"
    return read_csv(spark, path).cache()

@pytest.fixture(scope="session")
def blk_selection_for_eng_model_features_df(
    spark: SparkSession, args: ArgType, testbed: str
) -> DataFrame:
    path = args.artifacts_home + "src_data/blk_selection_for_eng_model_features.csv"
    return read_csv(spark, path).cache()

@pytest.fixture(scope="session")
def soc_ocv_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    path = args.artifacts_home + "src_data/soc_ocv_mapping.csv"
    return read_csv(spark, path).cache()

# ######## xEV Cache dataframes ###############################################
@pytest.fixture(scope="session")
def add_data_conv_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "xev.add_data_conv"
        add_data_conv_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        add_data_conv_df = spark.table(args.xev_db + "." + C.ADD_DATA_CONV_SRC_ENTITY)
    else:
        raise RuntimeError
    return add_data_conv_df

@pytest.fixture(scope="session")
def blk_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    # careful: in some columns, NULLs are represented by the string "UBK"
    if testbed == "local":
        file = "xev.blk"
        blk_df = (
            read_test_data(spark, args, file, encrypted=True)
            .select(C.BLK_SRC_COLS)
            .cache()
        )
    elif testbed == "regenerate_testdata":
        # We go for blk_hist because a lot of test cases have aged out of blk
        blk_df = spark.table(args.xev_db + "." + C.BLK_HIST_SRC_ENTITY)
    else:
        raise RuntimeError
    return blk_df

@pytest.fixture(scope="session")
def vehicle_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "xev.vehicle"
        vehicle_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        vehicle_df = spark.table(args.xev_db + "." + C.VEHICLE_SRC_ENTITY)
    else:
        raise RuntimeError
    return vehicle_df

@pytest.fixture(scope="session")
def dtc_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "xev.dtc"
        dtc_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        dtc_df = spark.table(args.xev_db + "." + C.DTC_SRC_ENTITY)
    else:
        raise RuntimeError
    return dtc_df

@pytest.fixture(scope="session")
def warranty_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "xev.warranty"
        warranty_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        warranty_df = spark.table(args.xev_db + "." + C.WARRANTY_SRC_ENTITY)
    else:
        raise RuntimeError
    return warranty_df

@pytest.fixture(scope="session")
def ecu_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "xev.ecu"
        ecu_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        ecu_df = spark.table(args.xev_db + "." + C.ECU_SRC_ENTITY)
    else:
        raise RuntimeError
    return ecu_df

# ######## afm2 computed dataframes ###########################################
@pytest.fixture(scope="session")
def afm_vehicle_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.vehicle"
        afm_vehicle_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        afm_vehicle_df = spark.table(args.afm_db + "." + C.VEHICLE_SRC_ENTITY)
    else:
        raise RuntimeError
    return afm_vehicle_df

@pytest.fixture(scope="session")
def battery_diag_df(
    spark: SparkSession, args: ArgType, testbed: str, diagnoses_for_test_df: DataFrame
) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.battery_diag"
        # https://stackoverflow.com/questions/38276588/config-file-to-define-json-schema-structure-in-pyspark/38276795#38276795
        battery_diag_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        battery_diag_df = (
            spark.table(args.afm_db + "." + C.BATTERY_DIAG_SRC_ENTITY)
            .transform(battery_diag_testdata(diagnoses_for_test_df))
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return battery_diag_df

@pytest.fixture(scope="session")
def battery_diag_cell_df(
    spark: SparkSession, args: ArgType, testbed: str, diagnoses_for_test_df: DataFrame
) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.battery_diag_cell"
        battery_diag_cell_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        battery_diag_cell_df = (
            spark.table(args.afm_db + "." + C.BATTERY_DIAG_CELL_SRC_ENTITY)
            .transform(battery_diag_cell_testdata(diagnoses_for_test_df))
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return battery_diag_cell_df

@pytest.fixture(scope="session")
def engineering_model_df(
    spark: SparkSession, args: ArgType, testbed: str, diagnoses_for_test_df: DataFrame
) -> DataFrame:
    if testbed == "local":
        file = "afm_forecast_dev.engineering_model"
        engineering_model_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        engineering_model_df = (
            spark.table(
                args.afm_forecast_db + "." + C.ENGINEERING_MODEL_LATEST_SRC_ENTITY
            )
            .transform(engineering_model_testdata(diagnoses_for_test_df))
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return engineering_model_df

@pytest.fixture(scope="session")
def forecast_diag_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.forecast_diag"
        forecast_diag_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        forecast_diag_df = (
            spark.table(args.afm_db + "." + C.FORECAST_DIAG_SRC_ENTITY)
            .transform(forecast_diag_testdata())
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return forecast_diag_df

@pytest.fixture(scope="session")
def model_overview_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_forecast_dev.model_overview"
        model_overview_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        model_overview_df = (
            spark.table(args.afm_forecast_db + "." + C.MODEL_OVERVIEW_SRC_ENTITY)
            .transform(model_overview_testdata())
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return model_overview_df

@pytest.fixture(scope="session")
def forecast_diag_ml_df(
    spark: SparkSession, args: ArgType, testbed: str, diagnoses_for_test_df: DataFrame
) -> DataFrame:
    if testbed == "local":
        file = "afm_forecast_dev.forecast_diag_ml"
        forecast_diag_ml_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        forecast_diag_ml_df = (
            spark.table(args.afm_forecast_db + "." + C.FORECAST_DIAG_ML_SRC_ENTITY)
            .transform(forecast_diag_ml_testdata(diagnoses_for_test_df))
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return forecast_diag_ml_df

@pytest.fixture(scope="session")
def forecast_validation_df(
    spark: SparkSession, args: ArgType, testbed: str, diagnoses_for_test_df: DataFrame
) -> DataFrame:
    if testbed == "local":
        file = "afm_forecast_dev.forecast_validation"
        forecast_validation_df = read_test_data(
            spark, args, file, encrypted=True
        ).cache()
    elif testbed == "regenerate_testdata":
        forecast_validation_df = (
            spark.table(args.afm_forecast_db + "." + C.FORECAST_VALIDATION_SRC_ENTITY)
            .transform(forecast_validation_testdata(diagnoses_for_test_df))
            .transform(clear_computation_graph(spark, args.tmp_dir, args.testbed))
        )
    else:
        raise RuntimeError
    return forecast_validation_df

@pytest.fixture(scope="session")
def afm_dtc_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.dtc"
        afm_dtc_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        afm_dtc_df = spark.table(args.afm_db + "." + C.DTC_SRC_ENTITY)
    else:
        raise RuntimeError
    return afm_dtc_df

@pytest.fixture(scope="session")
def afm_warranty_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.warranty"
        afm_warranty_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        afm_warranty_df = spark.table(args.afm_db + "." + C.WARRANTY_SRC_ENTITY)
    else:
        raise RuntimeError
    return afm_warranty_df

@pytest.fixture(scope="session")
def battery_type_df(spark: SparkSession, args: ArgType, testbed: str) -> DataFrame:
    if testbed == "local":
        file = "afm_dev.battery_type"
        battery_type_df = read_test_data(spark, args, file, encrypted=True).cache()
    elif testbed == "regenerate_testdata":
        battery_type_df = spark.table(args.afm_db + "." + C.BATTERY_TYPE_SRC_ENTITY)
    else:
        raise RuntimeError
    return battery_type_df
hat Kontextmenü


hat Kontextmenü
